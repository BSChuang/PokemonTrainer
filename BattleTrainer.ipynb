{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO CREATE CLEANED CSV FILES\n",
    "\"\"\" pokedf = None\n",
    "with open('pokemon-data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    pokemon = []\n",
    "    colNames = []\n",
    "    for row in reader:\n",
    "        pokemon.append(' '.join(row).split(';'))\n",
    "    colNames = pokemon[0]\n",
    "    pokemon.pop(0)\n",
    "    pokedf = pd.DataFrame(pokemon, columns=colNames)\n",
    "\n",
    "pokedf.drop(['Tier', 'Next Evolution(s)'], axis=1, inplace=True)\n",
    "pokedf.to_csv('pokemon-data-clean.csv', index=False) \"\"\"\n",
    "\n",
    "\"\"\" movedf = pd.read_csv('move-data.csv')\n",
    "movedf.drop(['Index', 'Generation'], axis=1, inplace=True)\n",
    "movedf.to_csv('move-data-clean.csv', index=False)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(arr):\n",
    "    newArr = []\n",
    "    for move in arr:\n",
    "        newName = (move.replace('-', '')).replace(\"'\", '') # Remove dashes\n",
    "        if movedf[movedf['Name'] == newName]['Category'].values[0] == 'Status': # Remove status moves\n",
    "            continue\n",
    "\n",
    "        # Remove moves with no power or accuracy\n",
    "        if np.isnan(movedf[movedf['Name'] == newName]['Power'].values[0]) or np.isnan(movedf[movedf['Name'] == newName]['Accuracy'].values[0]):\n",
    "            continue\n",
    "        \n",
    "        newArr.append(newName) \n",
    "    return newArr \n",
    "\n",
    "def standardize(row):\n",
    "    factor = 600 / (row['HP'] + row['Attack'] + row['Defense'] + row['Special Attack'] + row['Special Defense'] + row['Speed'])\n",
    "    row['HP'] = math.floor(factor * row['HP'])\n",
    "    row['Attack'] = math.floor(factor * row['Attack'])\n",
    "    row['Defense'] = math.floor(factor * row['Defense'])\n",
    "    row['Special Attack'] = math.floor(factor * row['Special Attack'])\n",
    "    row['Special Defense'] = math.floor(factor * row['Special Defense'])\n",
    "    row['Speed'] = math.floor(factor * row['Speed'])\n",
    "    return row\n",
    "\n",
    "movedf = pd.read_csv('move-data-clean.csv')\n",
    "movedf['Name'] = movedf['Name'].apply(lambda x: (x.replace('-', '')).replace(\"'\", '')) # Remove dashes (like in Double-Edge)\n",
    "movedf['Power'] = movedf['Power'].apply(lambda x: int(x) if x != 'None' else None)\n",
    "movedf['Accuracy'] = movedf['Accuracy'].apply(lambda x: int(x) if x != 'None' else None)\n",
    "\n",
    "pokedf = pd.read_csv('pokemon-data-clean.csv')\n",
    "for col in ['Types', 'Abilities', 'Moves']: # Turn the types, abilities, and moves from a string to a list\n",
    "    pokedf[col] = pokedf[col].apply(ast.literal_eval)\n",
    "pokedf = pokedf.apply(standardize, axis=1)\n",
    "pokedf['Moves'] = pokedf['Moves'].apply(cleanup)\n",
    "\n",
    "typedf = pd.read_csv('type-data-clean.csv', index_col=0)\n",
    "\n",
    "allTypes = ['Normal', 'Fire', 'Water', 'Electric', 'Grass', 'Ice', 'Fighting', 'Poison', 'Ground', 'Flying', 'Psychic', 'Bug', 'Rock', 'Ghost', 'Dragon', 'Dark', 'Steel', 'Fairy']\n",
    "allCategories = ['Physical', 'Special']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataframe is formatted correctly\n",
    "print(pokedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "#from tf_agents.environments import trajectory\n",
    "from tf_agents.environments import wrappers\n",
    "#from tf_agents.metrics import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "source": [
    "## Experiment 2: Pokemon Battle\n",
    "### Create two agents to pokemon battle and ultimately become the region's best pokemon champions\n",
    "1) Create a (simplified) environment where trainers may battle one another\n",
    "\n",
    "2) Use tensorflow/tf-agents to create agent\n",
    "\n",
    "3) Train the agent against random, then against algorithm, then itself possibly\n",
    "\n",
    "4) Result: agent who perform the best given its pokemon and abilities. Pokemon can be swapped out\n",
    "\n",
    "5) Constraints: no status effects (dataset does not have the effects of status effects). No items\n",
    "\n",
    "5) Stretch goals: agents take turns picking 6 pokemon to battle each other, then pick the pokemon's abilities \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Battle helper functions (Ignoring abilities and pp)\n",
    "# Pokemon: [types, hp, attack, defense, special attack, special defense, speed, move1type, move1category, ..., move1accuracy, move2type, ..., move4accuracy]\n",
    "# Moves: [type, category, pp, power, accuracy]\n",
    "# one-hot encode: types, movetype, category \n",
    "\"\"\" \n",
    "print(movedf.loc[0].values)\n",
    "print(typedf.loc['Water', 'Fire']) \"\"\"\n",
    "def calcDamage(poke1, poke2, move):\n",
    "    move = poke1[24+23*move:24+23*(move+1)]\n",
    "    attack = poke1[19] if move[18] == 'Physical' else poke1[21]\n",
    "    defense = poke2[20] if move[18] == 'Physical' else poke2[22]\n",
    "    modifier = 2\n",
    "    types = onehotToType(poke2[:18])\n",
    "    for type in types:\n",
    "        modifier *= typedf.loc[onehotToType(move[:18]), type]\n",
    "    damage = ((2/5 + 2) * move[20] * attack / defense / 50 + 2) * modifier# * (move[21] > np.random.randint(1, 101))\n",
    "    return math.ceil(damage)\n",
    "\n",
    "def typeToOneHot(types): # Input: list of types, output: one hot array where types are 1, else 0\n",
    "    onehot = [0] * len(allTypes)\n",
    "    for type in types:\n",
    "        onehot[allTypes.index(type)] = 1\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "def onehotToType(onehot): # Input onehot of types, output: list of types\n",
    "    onehot = np.array(onehot)\n",
    "    types = np.array(allTypes)\n",
    "    return types[np.where(onehot == 1)]\n",
    "\n",
    "def randPokemon():\n",
    "    poke = pokedf[:20].sample(1).values[0]\n",
    "    poke = np.delete(poke, 2) # Remove abilties\n",
    "    poke[8] = random.sample(poke[8], 4) # Choose 4 random moves for the pokemon\n",
    "    return poke\n",
    "\n",
    "\n",
    "def moveToVector(move): # Input: move name, output: move vector (movetype (one-hot), move category (one-hot), pp, power, accuracy)\n",
    "    move = movedf[movedf['Name'] == move]\n",
    "    moveType = typeToOneHot([move['Type'].values[0]])\n",
    "    moveCategory = [1, 0] if move['Category'].values[0] == 'Physical' else [0, 1]\n",
    "    moveOthers = [move['PP'].values[0], move['Power'].values[0], move['Accuracy'].values[0]]\n",
    "    return np.concatenate((moveType, moveCategory, moveOthers))\n",
    "\n",
    "\n",
    "def pokeToVector(poke):\n",
    "    pokeVec = np.concatenate((typeToOneHot(poke[1]), poke[2:8]))\n",
    "\n",
    "    for move in poke[8]:\n",
    "        pokeVec = np.concatenate((pokeVec, moveToVector(move)))\n",
    "    \n",
    "    return pokeVec.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poke1 = np.array(['Abomasnow', ['Grass', 'Ice'], 90, 92, 75, 92, 85, 60, ['Ice Punch', 'Powder Snow', 'Rock Slide', 'Razor Leaf']])\n",
    "class PokemonEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=3, name='action') # Action is single int from 0-3 signifying chosen move\n",
    "    self._observation_spec = array_spec.BoundedArraySpec( # Observation: [poke1, poke2[type and stats]] OR [poke1type, poke2type, poke1move1type, poke1move2type, poke1move3type, poke1move4type]\n",
    "        shape=(140,), dtype=np.int32, name='observation')\n",
    "    \n",
    "    self.poke1 = randPokemon()\n",
    "    self.poke2 = randPokemon()\n",
    "\n",
    "    self.poke1Vec = pokeToVector(self.poke1)\n",
    "    self.poke2Vec = pokeToVector(self.poke2)\n",
    "    self._state = np.concatenate((self.poke1Vec, self.poke2Vec[:24])) # State: poke1, poke2[type and stats]\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self.poke1 = randPokemon()\n",
    "    self.poke2 = randPokemon()\n",
    "\n",
    "    self.poke1Vec = pokeToVector(self.poke1)\n",
    "    self.poke2Vec = pokeToVector(self.poke2)\n",
    "\n",
    "    self._state = np.concatenate((self.poke1Vec, self.poke2Vec[:24]))\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(self._state)\n",
    "\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    poke2StartHP = self.poke2Vec[18]\n",
    "    self.poke2Action = random.randint(0, 3)\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if self._state[18] == 0 or self._state[134] == 0: # If either have fainted, end episode\n",
    "      self._episode_ended = True\n",
    "    else:\n",
    "      if action == 0 or action == 1 or action == 2 or action == 3: # If action is valid\n",
    "        if self.poke1Vec[23] > self.poke2Vec[23]:\n",
    "          order = {'first': self.poke1Vec, 'second': self.poke2Vec, 'firstAction': action, 'secondAction': self.poke2Action}\n",
    "        else:\n",
    "          order = {'first': self.poke2Vec, 'second': self.poke1Vec, 'firstAction': self.poke2Action, 'secondAction': action}\n",
    "\n",
    "        firstDamage = calcDamage(order['first'], order['second'], order['firstAction']) # Get damage\n",
    "        order['second'][18] = max(order['second'][18] - firstDamage, 0) # Apply damage\n",
    "        if order['second'][18] > 0: # If poke2 is still alive\n",
    "          secondDamage = calcDamage(order['second'], order['first'], order['secondAction']) # Get damage\n",
    "          order['first'][18] = max(order['first'][18] - secondDamage, 0) # Apply damage\n",
    "          if order['first'][18] == 0: # If poke1 fainted end episode\n",
    "            self._episode_ended = True\n",
    "        else:\n",
    "          self._episode_ended = True\n",
    "\n",
    "    # If neither have fainted, remove 1 pp from each pokemon's move\n",
    "    self.poke1Vec[24+(23*action)+20] -= 1\n",
    "    self.poke1Vec[24+(23*self.poke2Action)+20] -= 1\n",
    "\n",
    "    self._state = np.concatenate((self.poke1Vec, self.poke2Vec[:24]))\n",
    "    poke2HPDiff = poke2StartHP - self.poke2Vec[18]\n",
    "    if self._episode_ended:\n",
    "      if self.poke1Vec[18] > 0 and self.poke2Vec[18] == 0:\n",
    "        reward = 50\n",
    "      elif self.poke1Vec[18] == 0 and self.poke2Vec[18] > 0:\n",
    "        reward = -50\n",
    "      else:\n",
    "        reward = 0\n",
    "      reward += poke2HPDiff # Extra reward based on damage dealt\n",
    "      return ts.termination(self._state, reward)\n",
    "    else:\n",
    "      return ts.transition(self._state, reward=poke2HPDiff, discount=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PokemonEnv()\n",
    "utils.validate_py_environment(env, episodes=5)\n",
    "\n",
    "poke1Prev, poke2Prev = env.poke1Vec[18], env.poke2Vec[18]\n",
    "env._step(0)\n",
    "poke1Next, poke2Next = env.poke1Vec[18], env.poke2Vec[18]\n",
    "print(env.poke1[0], \"used\", env.poke1[8][0], \"|\", poke2Prev, \"->\", poke2Next)\n",
    "print(env.poke2[0], \"used\", env.poke2[8][env.poke2Action], \"|\", poke1Prev, \"->\", poke1Next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = wrappers.TimeLimit(PokemonEnv(), duration=100)\n",
    "eval_py_env = wrappers.TimeLimit(PokemonEnv(), duration=100)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "num_iterations = 75000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 128  # @param\n",
    "learning_rate = 1e-5  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 2  # @param\n",
    "eval_interval = 1000  # @param\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        #td_errors_loss_fn = dqn_agent.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "    \n",
    "iterator = iter(dataset)\n",
    "\n",
    "train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            train_env,\n",
    "            collect_policy,\n",
    "            observers=replay_observer + train_metrics,\n",
    "    num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_len = []\n",
    "\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = tf_agent.train(experience=experience)\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "        episode_len.append(train_metrics[3].result().numpy())\n",
    "        print('Average episode length: {}'.format(train_metrics[3].result().numpy()))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "plt.plot(episode_len)\n",
    "plt.show()"
   ]
  }
 ]
}