{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Experiment 2: Simple pokemon battle (AKA advanced rock paper scissors)\n",
    "### Create an agent to choose the best move given its move types and its opponent's type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TO CREATE CLEANED CSV FILES\n",
    "\"\"\" pokedf = None\n",
    "with open('pokemon-data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    pokemon = []\n",
    "    colNames = []\n",
    "    for row in reader:\n",
    "        pokemon.append(' '.join(row).split(';'))\n",
    "    colNames = pokemon[0]\n",
    "    pokemon.pop(0)\n",
    "    pokedf = pd.DataFrame(pokemon, columns=colNames)\n",
    "\n",
    "pokedf.drop(['Tier', 'Next Evolution(s)'], axis=1, inplace=True)\n",
    "pokedf.to_csv('pokemon-data-clean.csv', index=False) \"\"\"\n",
    "\n",
    "\"\"\" movedf = pd.read_csv('move-data.csv')\n",
    "movedf.drop(['Index', 'Generation'], axis=1, inplace=True)\n",
    "movedf.to_csv('move-data-clean.csv', index=False)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(arr):\n",
    "    newArr = []\n",
    "    for move in arr:\n",
    "        newName = (move.replace('-', '')).replace(\"'\", '') # Remove dashes\n",
    "        if movedf[movedf['Name'] == newName]['Category'].values[0] == 'Status': # Remove status moves\n",
    "            continue\n",
    "\n",
    "        # Remove moves with no power or accuracy\n",
    "        if np.isnan(movedf[movedf['Name'] == newName]['Power'].values[0]) or np.isnan(movedf[movedf['Name'] == newName]['Accuracy'].values[0]):\n",
    "            continue\n",
    "        \n",
    "        newArr.append(newName) \n",
    "    return newArr \n",
    "\n",
    "def standardize(row):\n",
    "    factor = 600 / (row['HP'] + row['Attack'] + row['Defense'] + row['Special Attack'] + row['Special Defense'] + row['Speed'])\n",
    "    row['HP'] = math.floor(factor * row['HP'])\n",
    "    row['Attack'] = math.floor(factor * row['Attack'])\n",
    "    row['Defense'] = math.floor(factor * row['Defense'])\n",
    "    row['Special Attack'] = math.floor(factor * row['Special Attack'])\n",
    "    row['Special Defense'] = math.floor(factor * row['Special Defense'])\n",
    "    row['Speed'] = math.floor(factor * row['Speed'])\n",
    "    return row\n",
    "\n",
    "movedf = pd.read_csv('move-data-clean.csv')\n",
    "movedf['Name'] = movedf['Name'].apply(lambda x: (x.replace('-', '')).replace(\"'\", '')) # Remove dashes (like in Double-Edge)\n",
    "movedf['Power'] = movedf['Power'].apply(lambda x: int(x) if x != 'None' else None)\n",
    "movedf['Accuracy'] = movedf['Accuracy'].apply(lambda x: int(x) if x != 'None' else None)\n",
    "\n",
    "pokedf = pd.read_csv('pokemon-data-clean.csv')\n",
    "for col in ['Types', 'Abilities', 'Moves']: # Turn the types, abilities, and moves from a string to a list\n",
    "    pokedf[col] = pokedf[col].apply(ast.literal_eval)\n",
    "pokedf = pokedf.apply(standardize, axis=1)\n",
    "pokedf['Moves'] = pokedf['Moves'].apply(cleanup)\n",
    "\n",
    "typedf = pd.read_csv('type-data-clean.csv', index_col=0)\n",
    "\n",
    "allTypes = ['Normal', 'Fire', 'Water', 'Electric', 'Grass', 'Ice', 'Fighting', 'Poison', 'Ground', 'Flying', 'Psychic', 'Bug', 'Rock', 'Ghost', 'Dragon', 'Dark', 'Steel', 'Fairy']\n",
    "allCategories = ['Physical', 'Special']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataframe is formatted correctly\n",
    "print(pokedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import py_environment\n",
    "#from tf_agents.environments import trajectory\n",
    "from tf_agents.environments import wrappers\n",
    "#from tf_agents.metrics import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Battle helper functions (Ignoring abilities and pp)\n",
    "# Pokemon: [types, hp, attack, defense, special attack, special defense, speed, move1type, move1category, ..., move1accuracy, move2type, ..., move4accuracy]\n",
    "# Moves: [type, category, pp, power, accuracy]\n",
    "# one-hot encode: types, movetype, category \n",
    "\"\"\" \n",
    "print(movedf.loc[0].values)\n",
    "print(typedf.loc['Water', 'Fire']) \"\"\"\n",
    "def calcDamage(poke1, poke2, move):\n",
    "    move = poke1[24+23*move:24+23*(move+1)]\n",
    "    attack = poke1[19] if move[18] == 'Physical' else poke1[21]\n",
    "    defense = poke2[20] if move[18] == 'Physical' else poke2[22]\n",
    "    modifier = 2\n",
    "    types = onehotToType(poke2[:18])\n",
    "    for type in types:\n",
    "        modifier *= typedf.loc[onehotToType(move[:18]), type]\n",
    "    damage = ((2/5 + 2) * move[20] * attack / defense / 50 + 2) * modifier # * (move[21] > np.random.randint(1, 101))\n",
    "    return math.ceil(damage)\n",
    "\n",
    "def typeToOneHot(types): # Input: list of types, output: one hot array where types are 1, else 0\n",
    "    onehot = [0] * len(allTypes)\n",
    "    for type in types:\n",
    "        onehot[allTypes.index(type)] = 1\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "def onehotToType(onehot): # Input onehot of types, output: list of types\n",
    "    onehot = np.array(onehot)\n",
    "    types = np.array(allTypes)\n",
    "    return types[np.where(onehot == 1)]\n",
    "\n",
    "def randPokemon():\n",
    "    poke = pokedf[:20].sample(1).values[0]\n",
    "    poke = np.delete(poke, 2) # Remove abilties\n",
    "    poke[8] = random.sample(poke[8], 4) # Choose 4 random moves for the pokemon\n",
    "    return poke\n",
    "\n",
    "\n",
    "def moveToVector(move): # Input: move name, output: move vector (movetype (one-hot), move category (one-hot), pp, power, accuracy)\n",
    "    move = movedf[movedf['Name'] == move]\n",
    "    moveType = typeToOneHot([move['Type'].values[0]])\n",
    "    moveCategory = [1, 0] if move['Category'].values[0] == 'Physical' else [0, 1]\n",
    "    moveOthers = [move['PP'].values[0], move['Power'].values[0], move['Accuracy'].values[0]]\n",
    "    return np.concatenate((moveType, moveCategory, moveOthers))\n",
    "\n",
    "\n",
    "def pokeToVector(poke):\n",
    "    pokeVec = np.concatenate((typeToOneHot(poke[1]), poke[2:8]))\n",
    "\n",
    "    for move in poke[8]:\n",
    "        pokeVec = np.concatenate((pokeVec, moveToVector(move)))\n",
    "    \n",
    "    return pokeVec.astype(np.int32)\n",
    "\n",
    "class PokemonEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):\n",
    "    self.round = 0\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=3, name='action') # Action is single int from 0-3 signifying chosen move\n",
    "    self._observation_spec = array_spec.BoundedArraySpec( # Observation: [poke2type (18), poke1move1type (18), poke1move2type (18), poke1move3type (18), poke1move4type (18)]\n",
    "        shape=(1, 90,), dtype=np.int32, name='observation')\n",
    "    \n",
    "    self.poke1 = randPokemon()\n",
    "    self.poke2 = randPokemon()\n",
    "\n",
    "    self.poke1Vec = pokeToVector(self.poke1)\n",
    "    self.poke2Vec = pokeToVector(self.poke2)\n",
    "\n",
    "    moves = []\n",
    "    for move in self.poke1[8]:\n",
    "        moves = np.concatenate((moves, moveToVector(move)[:18]))\n",
    "\n",
    "    self._state = np.concatenate((self.poke2Vec[:18], moves)).astype(np.int32) # State: poke1, poke2[type and stats]\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self.round = 0\n",
    "    self.poke1 = randPokemon()\n",
    "    self.poke2 = randPokemon()\n",
    "\n",
    "    self.poke1Vec = pokeToVector(self.poke1)\n",
    "    self.poke2Vec = pokeToVector(self.poke2)\n",
    "\n",
    "    moves = []\n",
    "    for move in self.poke1[8]:\n",
    "        moves = np.concatenate((moves, moveToVector(move)[:18]))\n",
    "\n",
    "    self._state = np.concatenate((self.poke2Vec[:18], moves)).astype(np.int32) # State: poke2 type, move1 type, move2 type, ..., move4 type\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(self._state)\n",
    "\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "\n",
    "    if action == 0 or action == 1 or action == 2 or action == 3: # If action is valid\n",
    "        pokeTypes = onehotToType(self._state[:18])\n",
    "        moveType = onehotToType(self._state[18 + 18*action: 18 + 18 * (action + 1)])[0]\n",
    "        reward = 100\n",
    "        for pokeType in pokeTypes:\n",
    "          reward *= typedf.loc[moveType, pokeType]\n",
    "      \n",
    "    self._episode_ended = True\n",
    "    if self._episode_ended:\n",
    "      return ts.termination(self._state, reward)\n",
    "    else:\n",
    "      return ts.transition(self._state, reward=reward, discount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PokemonEnv()\n",
    "utils.validate_py_environment(env, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = wrappers.TimeLimit(PokemonEnv(), duration=100)\n",
    "eval_py_env = wrappers.TimeLimit(PokemonEnv(), duration=100)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "num_iterations = 75000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 128  # @param\n",
    "learning_rate = 1e-5  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 2  # @param\n",
    "eval_interval = 1000  # @param\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        #td_errors_loss_fn = dqn_agent.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "    \n",
    "iterator = iter(dataset)\n",
    "\n",
    "train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            train_env,\n",
    "            collect_policy,\n",
    "            observers=replay_observer + train_metrics,\n",
    "    num_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_len = []\n",
    "\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = tf_agent.train(experience=experience)\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "        episode_len.append(train_metrics[3].result().numpy())\n",
    "        print('Average episode length: {}'.format(train_metrics[3].result().numpy()))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "plt.plot(episode_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_policy = agent.collect_policy\n",
    "saver = PolicySaver(my_policy, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy = tf.compat.v2.saved_model.load('policy_move')\n",
    "pokeEnv = PokemonEnv()\n",
    "timestep = pokeEnv.reset()\n",
    "print(timestep)\n",
    "print(saved_policy.action(timestep))\n",
    "\n",
    "#action = saved_policy.action(timestep)"
   ]
  }
 ]
}